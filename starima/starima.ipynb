{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUINDO PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "from grid import *\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Função para carregar e preparar dados (ajuste com seus dados)\n",
    "def load_and_prepare_data(file_path):\n",
    "    cols_wanted = ['data_hora_fato', 'natureza_descricao', 'numero_latitude', 'numero_longitude']\n",
    "    df = pd.read_csv(file_path, usecols=cols_wanted)\n",
    "    df['data_hora_fato'] = pd.to_datetime(df['data_hora_fato'], errors='coerce')\n",
    "    df=df.dropna()\n",
    "    df = df[df['data_hora_fato'].dt.year == 2024]\n",
    "    df= df[df['natureza_descricao']=='FURTO']\n",
    "    # Ordenar o DataFrame por data\n",
    "    df = df.sort_values(by='data_hora_fato')\n",
    "    \n",
    "    # Filtrar os primeiros 7 meses\n",
    "    start_date = df['data_hora_fato'].min()\n",
    "    end_date = start_date + pd.DateOffset(months=7)\n",
    "    df = df[(df['data_hora_fato'] >= start_date) & (df['data_hora_fato'] < end_date)]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Função para agregar os dados por célula e por dia\n",
    "def aggregate_data(data, grid):\n",
    "    \"\"\"\n",
    "    Agrega os dados por célula do grid e por dia.\n",
    "    \"\"\"\n",
    "    # Verificar se a coluna de coordenadas existe\n",
    "    if 'numero_longitude' not in data.columns or 'numero_latitude' not in data.columns:\n",
    "        raise ValueError(\"Colunas 'numero_longitude' e 'numero_latitude' estão ausentes no DataFrame.\")\n",
    "    \n",
    "    # Converter a coluna de data para datetime\n",
    "    data['data_hora_fato'] = pd.to_datetime(data['data_hora_fato'], errors='coerce')\n",
    "    \n",
    "    # Criar geometria com base em latitude e longitude\n",
    "    data['geometry'] = gpd.points_from_xy(data['numero_longitude'], data['numero_latitude'])\n",
    "    data_gdf = gpd.GeoDataFrame(data, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    # Verificar se o CRS do grid está alinhado com os pontos\n",
    "    if grid.crs.to_string() != data_gdf.crs.to_string():\n",
    "        grid = grid.to_crs(data_gdf.crs)\n",
    "    \n",
    "    # Realizar junção espacial para associar os pontos às células do grid\n",
    "    joined = gpd.sjoin(data_gdf, grid, how='inner', predicate='intersects')\n",
    "    \n",
    "    # Verificar se a junção retornou resultados\n",
    "    if joined.empty:\n",
    "        print(\"A junção espacial retornou um DataFrame vazio.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Garantir que o índice seja do tipo DatetimeIndex\n",
    "    joined['data_hora_fato'] = pd.to_datetime(joined['data_hora_fato'])\n",
    "    joined.set_index('data_hora_fato', inplace=True)\n",
    "    \n",
    "    # Agregar os dados por célula e por dia\n",
    "    grouped = joined.groupby(['cell', pd.Grouper(freq='D')]).size()\n",
    "\n",
    "    # Verificar se a agregação produziu resultados\n",
    "    if grouped.empty:\n",
    "        print(\"A agregação não produziu resultados.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Transformar os resultados em um DataFrame\n",
    "    aggregated_data = grouped.unstack(level=0, fill_value=0)\n",
    "    return aggregated_data\n",
    "\n",
    "# Função para dividir dados em treino e teste\n",
    "def train_test_split_data(data, test_size=7):\n",
    "    train_data = data.iloc[:-test_size]\n",
    "    test_data = data.iloc[-test_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Função para construir e treinar o modelo SARIMA\n",
    "def train_and_predict_sarima(train_data, steps=7):\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    \n",
    "    for cell in train_data.columns:\n",
    "        ts = train_data[cell]\n",
    "        \n",
    "        # Verifique se o índice é um DatetimeIndex\n",
    "        if not isinstance(ts.index, pd.DatetimeIndex):\n",
    "            ts.index = pd.to_datetime(ts.index)\n",
    "\n",
    "        if ts.sum() == 0:  # Se a série é composta apenas de zeros, prever zero\n",
    "            predictions.append([0] * steps)\n",
    "            real_values.append([0] * steps)\n",
    "            continue\n",
    "\n",
    "        # Treinar o modelo SARIMA\n",
    "        model = SARIMAX(ts, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        \n",
    "        # Fazer predições para os próximos 7 dias\n",
    "        forecast = model_fit.forecast(steps=steps)\n",
    "\n",
    "        # Garantir que a previsão tenha um índice de data\n",
    "        forecast.index = pd.date_range(start=ts.index[-1] + pd.Timedelta(days=1), periods=steps)\n",
    "        \n",
    "        predictions.append(forecast)\n",
    "        real_values.append(ts[-steps:].values)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions).T\n",
    "    real_values_df = pd.DataFrame(real_values).T\n",
    "    predictions_df.columns = train_data.columns\n",
    "    real_values_df.columns = train_data.columns\n",
    "    \n",
    "    return predictions_df, real_values_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 3064.74it/s]\n"
     ]
    }
   ],
   "source": [
    "grid_size = 800\n",
    "municipalities = [\"BELO HORIZONTE\"]\n",
    "grid = create_grid(grid_size, municipalities)\n",
    "\n",
    "# Carregamento e preparação dos dados\n",
    "file_path = '/home/amarante/Jobs/PM/dataset/dados_BH.csv'\n",
    "data = load_and_prepare_data(file_path)\n",
    "\n",
    "# Agregação dos dados no grid\n",
    "aggregated_data = aggregate_data(data, grid)\n",
    "# Ajustar o DataFrame para ter todas as células do grid, preenchendo com zeros onde necessário\n",
    "aggregated_data = aggregated_data.reindex(columns=grid['cell'], fill_value=0)\n",
    "# Separar treino e teste\n",
    "train_data, test_data = train_test_split_data(aggregated_data, test_size=30)\n",
    "predictions, real_values = train_and_predict_sarima(train_data, steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate_percentage = 0.1\n",
    "\n",
    "def hit_rate( grid_predicted, grid_test, ncells=0):\n",
    "    if not ncells:\n",
    "        num_cells = int(len(grid_predicted) * (hit_rate_percentage))\n",
    "        top_cells = grid_predicted.nlargest(num_cells, \"count\")\n",
    "    else:\n",
    "        top_cells = grid_predicted.nlargest(ncells, \"count\")\n",
    "\n",
    "    hit_rate = (\n",
    "        grid_test[grid_test[\"cell\"].isin(top_cells[\"cell\"])][\"count\"].sum()\n",
    "        / grid_test[\"count\"].sum()\n",
    "    )\n",
    "\n",
    "    return hit_rate\n",
    "\n",
    "def average_logarithmic_score( grid_predicted, grid_test):\n",
    "    val = grid_predicted[\"count\"].to_numpy()[\n",
    "        grid_test[grid_test[\"count\"] > 0][\"cell\"].values\n",
    "    ]\n",
    "    val[val < 0] = 0\n",
    "    zeros = (val == 0).sum()\n",
    "    val[val == 0] = 1\n",
    "    return np.mean(np.log(val)), zeros\n",
    "\n",
    "def mean_squared_error( grid_predicted, grid_test):\n",
    "    return np.mean((grid_predicted[\"count\"] - grid_test[\"count\"]) ** 2)\n",
    "\n",
    "def pai( grid_predicted, grid_test, ncells=0):\n",
    "\n",
    "    hr = hit_rate(grid_predicted, grid_test, ncells)\n",
    "\n",
    "    num_cells = int(len(grid_predicted) * (hit_rate_percentage))\n",
    "    grid_area = grid_size * grid_size\n",
    "\n",
    "    a = num_cells * grid_area\n",
    "    A = len(grid_predicted) * grid_area\n",
    "\n",
    "    return (hr / (a/A))\n",
    "\n",
    "def pei( grid_predicted, grid_test):\n",
    "\n",
    "    \n",
    "    num_cells = grid_test[grid_test[\"count\"] > 0][\"cell\"].nunique()\n",
    "    max_cells = int(len(grid_predicted) * (hit_rate_percentage))\n",
    "\n",
    "    num_cells = min(max_cells, num_cells)\n",
    "\n",
    "    pai_ = pai(grid_predicted, grid_test, ncells=num_cells)\n",
    "\n",
    "    grid_area = grid_size * grid_size\n",
    "    a = num_cells * grid_area\n",
    "    A = len(grid_predicted) * grid_area\n",
    "\n",
    "\n",
    "    return pai_ / (1/(a/A))\n",
    "\n",
    "def eval(grid_predicted, grid_test):\n",
    "\n",
    "    grid_predicted = grid_predicted.sort_values(\"cell\")\n",
    "    grid_test = grid_test.sort_values(\"cell\")\n",
    "\n",
    "    metrics = {\n",
    "        f\"HR({hit_rate_percentage*100}%)\": hit_rate(grid_predicted, grid_test),\n",
    "        \"ALS\": average_logarithmic_score(grid_predicted, grid_test)[0],\n",
    "        \"ALS_zeros\": average_logarithmic_score(grid_predicted, grid_test)[1],\n",
    "        \"MSE\": mean_squared_error(grid_predicted, grid_test),\n",
    "        \"PAI\": pai(grid_predicted, grid_test),\n",
    "        \"PEI\": pei(grid_predicted, grid_test)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR(10.0%)    0.653283\n",
       "ALS         -5.891289\n",
       "ALS_zeros    3.366667\n",
       "MSE          0.000013\n",
       "PAI          6.532834\n",
       "PEI          0.564818\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(30):\n",
    "    predictions_df = pd.DataFrame({\"count\":predictions.iloc[i]/predictions.iloc[i].sum()}).reset_index()\n",
    "    real_values_df = pd.DataFrame({\"count\":real_values.iloc[i]/real_values.iloc[i].sum()}).reset_index()\n",
    "    \n",
    "    res = eval(predictions_df, real_values_df)\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
